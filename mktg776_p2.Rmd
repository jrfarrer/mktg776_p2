---
title: "Dish Network Subscriber Acquisition"
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{hyperref}
- \usepackage{endnotes}
- \let\footnote=\endnote
date: '2017-04-05'
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 1
  bookdown::html_document2:
    css: style.css
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
subtitle: 'MTKG776: Applied Probability Models in Marketing'
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = 'center', size = 'small', fig.pos = 'H',
                      fig.width = 6, fig.height = 3.5)

knitr::knit_hooks$set(
          size = function(before, options, envir) {
                  if (before) return(paste0("\n \\", options$size, "\n\n"))
                  else return("\n\n \\normalsize \n")
                  }
          , inline = function(x) {if(!is.numeric(x)){ x }else{ prettyNum(round(x,2), big.mark=",") } }
          )
```

```{r report-load-packages, results = 'hide', include = FALSE}
pacman::p_load(tidyverse, forcats, pander, stringr, lubridate, jrfTools, extrafont, RcppRoll, ggrepel, tm, pdftools, tidyquant, forecast)
```

```{r report-additional-setup, include = FALSE}
options(scipen=999)
panderOptions('digits', 8)
panderOptions('round', 4)
panderOptions('keep.trailing.zeros', TRUE)
panderOptions('big.mark', ',')

model_dir <- "model_dir/"
viz_dir <- "viz/"
```

```{r report-wordcount}
fn_cound_words <- function(file) {
  txt <- pdf_text(file)
  corp1 <- VCorpus(VectorSource(txt))
  corp2 <- tm_map(corp1, stripWhitespace)
  corp3 <- tm_map(corp2, removePunctuation)
  corp4 <- tm_map(corp3, content_transformer(tolower))
  corp5 <- tm_map(corp4, removeNumbers)
  dtm <- TermDocumentMatrix(corp5)
  dtm %>% as.matrix() %>% sum()
}

if (file.exists(paste0("mktg776_p2.pdf"))) {
 n_words <- fn_cound_words(paste0("mktg776_p2.pdf")) 
} else {
  n_words <- 0
}
```

```{r load_dish_dataset}
dish <- read_csv("dish_data.csv") %>%
  mutate(quarter_date = yq(quarter))
```

```{r fn_negate}
fn_negate <- function(x) {
  if (x == Inf) {
    return(-Inf)
  } else if (x == -Inf) {
    return (Inf)
  } else {
    return(-x)
  }
}
```

```{r fn_create_covariate_df}
fn_create_covariate_df <- function(data, input_cov_names, cov_betas) {
  
  if (!is.null(input_cov_names)) {
    
    #Check that number of betas matches number of cov_names
    if (length(cov_betas) != length(input_cov_names)) {
      stop("Number of betas does not match number of cov_names")
    }
  
    data2 <- 
      data %>%
      mutate(isCov = TRUE) %>%
      unite(cov_char, one_of(input_cov_names), sep = ",", remove = FALSE) %>%
      mutate(
        cov_char_split = stringr::str_split(cov_char, ",")
        , covs = map(cov_char_split, as.numeric)
      ) %>%
      select(-cov_char, -cov_char_split)
      
  } else {
    data2 <- data %>% mutate(isCov = FALSE, covs = NA)
  }
  return(data2)
}
```

```{r fn_PT_less_t}
fn_PT_less_t <- function(Bt_cum, Bt_instant, isCov, r, alpha, heterogeneity) {
  
  if (heterogeneity == "gamma") {
    if (isCov) {
      PT_less_t <- 1 - (alpha / (alpha + Bt_cum))^r
    } else {
      PT_less_t <- 1 - (alpha / (alpha + Bt_instant))^r
    }
  } 
  
  return(PT_less_t)
}
```

```{r fn_data3}
fn_weibull_cov <- function(r, alpha, c, cov_betas, data2, cov_names, population, heterogeneity) {
  
  data3 <- 
    data2 %>%
    rowwise() %>%
    mutate(expBX  = if_else(isCov, exp(sum(cov_betas * covs)), 1.0)) %>%
    ungroup() %>%
    mutate(
      Bt_instant = (t^c - if_else(isCov, lag(t, default = 0)^c, 0)) * expBX
      , Bt_cum = cumsum(Bt_instant)
    ) %>%
    rowwise() %>%
    mutate(
      PT_less_t = map2_dbl(Bt_cum, Bt_instant, fn_PT_less_t, isCov, r, alpha, heterogeneity)
    ) %>%
    ungroup() %>%
    mutate(
       PT_t = PT_less_t - lag(PT_less_t, default = 0)
      , ll = x * log(PT_t)
    )
  
  return(data3)
}
  
```

```{r fn_wg_cov_ll}
fn_wg_cov_ll <- function(starting_values, data, cov_names, population, heterogeneity) {
  
  # Parameter Declaration
  r <- starting_values[1]
  alpha <- starting_values[2] 
  c <- starting_values[3];
  cov_betas <- starting_values[-c(1:3)]
  
  data2 <- fn_create_covariate_df(data, cov_names, cov_betas)
  
  data3 <- fn_weibull_cov(r, alpha, c, cov_betas, data2, cov_names, population, heterogeneity)
  
  last_ll <- (population - sum(data3$x)) * log(1 - tail(data3$PT_less_t, 1))
  ll <- sum(c(data3$ll, last_ll), na.rm = TRUE)

  neg_ll <- fn_negate(ll)
  return(neg_ll)
}
```

```{r fn_wg_cov_model}
fn_wg_cov_model <- function(model, data, cov_names, population, heterogeneity = c("gamma", "latent","2-segment gamma"), trace = FALSE) {
  
  if (!is.null(cov_names)) {
    cov_pars <- length(cov_names)
    init_par <- list(start =c(c(10, 10000, 1), rep(0.01, cov_pars)), lower = c(rep(0, 3), rep(-Inf, cov_pars)), 
                     upper = c(Inf, Inf, Inf, rep(Inf, cov_pars)))
  } else {
    cov_pars <- 0
    init_par <- list(start = rep(1, 3), lower = rep(0, 3), upper = rep(Inf, 3))
  }
  
  last_ll <- Inf 
  next_ll <- 1e10
  starting_values <- init_par$start
  while (next_ll < last_ll) {
    last_ll <- next_ll
    pars <- 
      nlminb(starting_values, fn_wg_cov_ll, lower = init_par$lower, upper = init_par$upper, 
                 data = data, cov_names = cov_names, population = population, heterogeneity = heterogeneity, 
                 control = list(eval.max = 500, iter.max = 500, x.tol = 1e-20,  trace = trace, 
                                abs.tol = 1e-20, rel.tol = 1e-15, x.tol = 1e-20,  xf.tol = 1e-20,
                                step.min = 1, step.max = 1, sing.tol = 1e-20))
    next_ll <- pars$objective
    starting_values <- pars$par
  }
  
  return(
    data_frame(model = model, r = pars$par[1], alpha = pars$par[2], c = pars$par[3], 
               cov_names = list(cov_names), cov_betas = list(pars$par[-c(1:3)]), params = (3 + cov_pars),
               population = population, heterogeneity = heterogeneity, ll = pars$objective)
  )
}
```

```{r downlaod_fred_metrics}
fred_metrics <-
  c("UMCSENT", "CANDH") %>% 
  tq_get(get = "economic.data", from = "1996-01-01")

scale_this <- function(x) as.vector(scale(x))
  
fred_metrics2 <-
  fred_metrics %>%
  mutate(
    quarter = quarter(date, with_year = TRUE)
    , quarter_date = yq(quarter)
  ) %>%
  group_by(symbol, quarter_date, quarter) %>%
  summarise(price = mean(price)) %>%
  group_by(symbol) %>%
  mutate(scaled_value = scale_this(price)) %>%
  ungroup() %>%
  select(quarter_date, quarter, symbol, scaled_value) %>%
  spread(symbol, scaled_value)
```

```{r seasonality}
ts_customers_acquired <- ts(dish$customers_acquired, frequency = 4)
decompose_acquisition <- stl(ts_customers_acquired, s.window = 7)

seasonality_df <- 
  decompose_acquisition$time.series %>% as.data.frame %>% as_tibble()

seasonality <- 
  data_frame(
    quarter_date = seq.Date(from = as.Date("1996-01-01"), to = as.Date("2016-12-31"), by = "quarter")
    , seasonality = scale_this(seasonality_df$seasonal)
  )
```

```{r slingbox_effect}
slingbox_effect <-
  data_frame(
    quarter_date = seq.Date(from = as.Date("1996-01-01"), to = as.Date("2016-12-31"), by = "quarter")
    , slingbox_effect = c(rep(0, 84-4-4), .1, .4, .8, 1, rep(0, 4))
  )
```

```{r netflix_stock}
netflix_financials <- tq_get("NFLX", get = "stock.prices", from = "1996-01-01", to = "2017-03-31")

netflix_stock <- 
  data_frame(
    quarter_date = seq.Date(from = as.Date("1996-01-01"), to = as.Date("2017-03-31"), by = "quarter")
  ) %>%
    left_join(
      netflix_financials %>%
        select(date, adjusted) %>%
        mutate(quarter_date = floor_date(date, unit = "quarter")) %>%
        group_by(quarter_date) %>%
        summarise(netflix_stock = mean(adjusted, na.rm = TRUE)) %>%
        ungroup() %>%
        mutate(netflix_stock = scale_this(netflix_stock)) 
      , by = "quarter_date"
    ) %>%
    replace_na(list(netflix_stock = 0))
```

```{r add_covariates_to_dish_data}
dish_model_data <-
  dish %>%
  select(-quarter) %>%
  left_join(fred_metrics2, by = c("quarter_date")) %>%
  left_join(seasonality, by = c("quarter_date")) %>%
  left_join(slingbox_effect, by = c("quarter_date")) %>%
  left_join(netflix_stock, by = c("quarter_date")) %>%
  rename(t = observation, x = customers_acquired)
```

```{r create_and_save_models, eval = FALSE}
covariates <- 
  dish_model_data %>%
    select(-t, -x, -quarter_date, -quarter) %>%
    names()

cov_list <- list()
for (i in 0:length(covariates)) {
  cov_list <- append(cov_list, combn(covariates, i, simplify = FALSE))
}

# Change string to detect to modify only certain models
cov_list_limited <- cov_list[str_detect(cov_list, ".")]
for (j in 1:length(cov_list_limited)) {
  if (length(cov_list_limited[[j]]) == 0) {
    model_name <- "No Covariates"
    model_filename <- "no_covs"
    cov_names <- NULL
  } else { 
    model_name <- paste0(cov_list_limited[[j]], collapse = " & ")
    model_filename <- paste0(cov_list_limited[[j]], collapse = "_")
    cov_names <- cov_list_limited[[j]]
  }
  
  tmp_df <- fn_wg_cov_model(model_name, dish_model_data, cov_names = cov_names, population = 60000, heterogeneity = "gamma")
  saveRDS(tmp_df, file = paste0(model_dir, model_filename, ".RDS"))
}
```

```{r load_all_models}
model_files <- list.files(path = model_dir, full.names = TRUE)
wg_cov_params <- lapply(model_files, readRDS) %>% bind_rows()
```

```{r results-by-quarter}
wg_cov_results <- 
  data_frame(quarter_date = seq.Date(from = as.Date("1996-01-01"), to = as.Date("2016-12-31"), by = "quarter")) %>%
  left_join(dish_model_data, by = c("quarter_date")) %>%
  crossing(model = wg_cov_params$model) %>%
  group_by(model) %>%
  nest() %>%
  left_join(wg_cov_params, by = "model") %>%
  mutate(data2 = pmap(list(data, cov_names,cov_betas), fn_create_covariate_df)) %>%
  mutate(data3 = pmap(list(r, alpha, c, cov_betas, data2, cov_names, population, heterogeneity), fn_weibull_cov)) %>%
  mutate(neg_ll = -1 * ll) %>%
  select(model, r, alpha, c, population, heterogeneity, params, neg_ll, data3) %>%
  unnest(data3) %>%
  mutate(
    expected_instant = population * PT_t
    , expected_cumulative = population * PT_less_t
    , ape = abs(expected_instant - x) / x
  ) %>%
  select(model, r, alpha, c, population, heterogeneity, params, neg_ll, ape, 
         quarter_date, t, expected_instant, expected_cumulative)
```

```{r model_level_summary}
wg_cov_summary <- 
  wg_cov_results %>% 
  group_by(model, params, neg_ll, population) %>%
  summarise(
      mdape = median(ape)
  )  %>%
  ungroup() %>%
  mutate(bic = -2 * neg_ll * 1000 + params * log(population * 1000)) %>%
  select(-population) %>%
  arrange(bic)
```

```{r four_quarters_to_forecast}
forecast_data <- 
  data_frame(
      x = rep(NA, 4)
    , t = 85:88
    , quarter_date = seq.Date(from = as.Date("2017-01-01"), to = as.Date("2017-12-31"), by = "quarter")
    , quarter = quarter(quarter_date, with_year = TRUE)
    , CANDH = c((fred_metrics2 %>% tail(1))$CANDH, 
              forecast(auto.arima(fred_metrics2$CANDH), 3)$mean)
    , UMCSENT = c((fred_metrics2 %>% tail(1))$UMCSENT,
                 forecast(auto.arima(fred_metrics2$UMCSENT), 3)$mean)
    , seasonality = scale_this(c(seasonality_df$seasonal, 
                        forecast(decompose_acquisition, h = 4)$seasonal)) %>% tail(4)
    , slingbox_effect = rep(0, 4)
    , netflix_stock = c((netflix_stock %>% tail(1))$netflix_stock, 
                        forecast(auto.arima(netflix_stock$netflix_stock), 3)$mean)
  )
```

```{r forecasted_data}
forecasted_data <- 
  bind_rows(dish_model_data, forecast_data) %>% 
  mutate(model = "CANDH & UMCSENT & seasonality & slingbox_effect & netflix_stock") %>%
  nest(-model) %>%
  left_join(wg_cov_params, by = "model") %>%
  mutate(data2 = pmap(list(data, cov_names,cov_betas), fn_create_covariate_df)) %>%
  mutate(data3 = pmap(list(r, alpha, c, cov_betas, data2, cov_names, population, heterogeneity), fn_weibull_cov)) %>%
  select(model, population, data3) %>%
  unnest(data3) %>%
  mutate(
    expected_instant = population * PT_t
    , expected_cumulative = population * PT_less_t
  ) %>%
  mutate(model = "Model") %>%
  select(model, quarter_date, Incremental = expected_instant, 
         Cumulative = expected_cumulative)
```

```{r dish_actual_and_forecast}
dish_actual <- 
  dish %>%
  mutate(
    Incremental = customers_acquired
    , Cumulative = cumsum(Incremental)
    , model = "Actual"
  ) %>%
  select(model, quarter_date, Incremental, Cumulative)

dish_actual_and_forecast <- 
  bind_rows(dish_actual, forecasted_data) %>%
  mutate(forecast = quarter_date >= as.Date("2017-01-1")) %>%
  gather(metric, value, -model, -quarter_date, -forecast) %>%
  mutate(metric = factor(metric, levels = c("Incremental", "Cumulative")))
```

**Word Count**: `r n_words`

# Executive Summary

In this analysis we attempt to identify and implement a timing model that will best predict Dish Network's subscriber acquisition in 2017. Our starting dataset is quarterly subscriber acquisition from 1996 to 2016. Using a Weibull distribution to explain a customer's time to subscribe we attempt to account for heterogeneity with segments (finite-mixture models), including hard-core never-acquirers, and a continuous Gamma distribution. We find that a Weibull-Gamma model performs well but alone is insufficient to explain Dish Network's customer acquisition. By adding covariates that describe macroeconomic trends, consumer sentiment, seasonality, and the competitor Netflix's performance, we improve our explanation of Dish' Network's customer acquisition. The plot below presents Dish's actual incremental subscriber acquisition by quarter and our model and forecast (dashed) for 2017.

```{r executive_summary_plot, fig.width = 6, fig.height = 3.4}
dish_actual_and_forecast %>%
  bind_rows(
    dish_actual_and_forecast %>% 
      filter(quarter_date == "2016-10-01" & model == "Model") %>% 
      mutate(quarter_date = quarter_date, forecast = TRUE)
  ) %>%
  filter(metric == "Incremental") %>%
  ggplot(aes(x = quarter_date, y = value, colour = model, linetype = forecast)) +
  geom_line() + 
  geom_point(size = .75) + 
  theme_jrf(users_v = "rstudio") +
  guides(linetype = FALSE) + 
  labs(title = "Dish Network Subscriber Acquisition", y = "Incremental Customers Acquired", 
       x = "Quarter", colour = NULL) +
  scale_y_continuous(labels = scales::comma) +
  scale_colour_manual(values = c(pal538[['red']], pal538[['blue']])) +
  scale_x_date(breaks = as.Date(c("1996-01-01", "2001-01-01", "2006-01-01", "2011-01-01", "2017-01-01")), 
               date_labels = "%Y")
```

# Analysis

## Objective

Our objective is to build a timing model to forecast the quarterly customer acquisitions for Dish Network in 2017.

## Candidate Models

The diagram below (Figure \@ref(fig:candidate-models-figure)) provides a framework and an assessment of the timing models considered for this analysis. As a baseline, individual-level model the exponential distribution was not considered because it has no duration dependence. A Weibull was used to allow for duration dependence, the probability of the customer signing up for Dish now, given that they have not signed up yet, to change over time. If heterogeneity were included via a gamma distribution of rate parameter $\lambda$, the exponential-gamma distribution (i.e. Pareto II) has a decreasing hazard function which is neither expected for the Dish product nor evident by growth rate in customer acquisition in the data. 

The remaining red X's represent models or factors that were attempted but were not selected in the final model. Finite-mixture models of Weibull distributions produced segments with nearly all 60M customers indicating there were not true segments, but rather the customer population was rather homogeneous. In addition, a finite-mixture model of Weibull-Gamma distribution with 2 segments produced one segment with nearly all the customers and another with none. The concept of hard-core never-acquirers was introduced with a vanilla Weibull and a Weibull-Gamma distribution, but both resulted in $\pi = 0$ and thus no evidence of a hard-core never-acquirer segment. Four categories of covariates were implemented: macro-trends, seasonality, firm-specific, and industry-specific. We found that the firm-specific covariate did not capture enough new information for forecasts to warrant inclusion.

```{r candidate-models-figure, fig.cap = 'Candidate Models', out.width = '100%'}
knitr::include_graphics(paste0(viz_dir, 'candidate_models.png'))
```


The resulting model is a Weibull-gamma (i.e. Burr XII) with covariates model that has a cumulative density function given by:

\begin{align}
\ P(T \le t) & = \int_{0}^{\infty} \Big(1 - e^{\lambda B(t)} \Big) \frac{\alpha^r \lambda^{r-1} e^{-\alpha \lambda}}{\Gamma(r)} d \lambda \\
 & = 1 - \Big( \frac{\alpha}{\alpha + B(t)}  \Big)^{r}
\end{align}

where

\begin{align}
\ B(t) = \sum_{i=1}^{t} \big( i^c - (i- 1)^c \big) e^{\boldsymbol{x}(i) \boldsymbol{\beta}}
\end{align}

## Covariates

The Weibull-Gamma (WG) model explains customer acquisition as each person in the population having some underlying, unobservable time to buy rate $\lambda$ and a hazard function that changes over time with a shape determined by $c$. Furthermore, the model assumes that the rate parameter $\lambda$ is distributed across the population according to a gamma distribution. Even with this individual-level story and expression of heterogeneity, we have reason to believe that Dish Network's customer acquisition may be influenced by the following external factors:

1. **Macro-Trends** - the performance of the US economy and how consumers feel
2. **Seasonality** - the WG does not distinguish Q2 to Q4, but consumers do
3. **Firm-Specific** - Dish may have taken actions (e.g. product launches) that can contribute to acquisition not governed by the WG
4. **Industry-Specific** - competitive forces or the changing TV environment may influence customer acquisition

So that each external factor, or covariate, can be compared all were scaled appropriately.

### Macro-Trends

To capture macro-trends, we explored different indices published on [FRED](https://fred.stlouisfed.org/)^[https://fred.stlouisfed.org/], the Federal Reserve Economic Data portal managed by the Federal Reserve Bank of St. Louis. We sought to find metrics that would influence Americans' propensity to start a new subscription service (likely transferring from another television service such as cable). We settled on two indices that capture slightly different phenomena:

1. [**CANDH**](https://fred.stlouisfed.org/series/CANDH)^[https://fred.stlouisfed.org/series/CANDH]: This index is a component of the Chicago Fed's National Activity Index, which is "a weighted average of 85 monthly indicators of national
economic activity".^[https://www.chicagofed.org/~/media/publications/cfnai/background/cfnai-background-pdf.pdf] There are multiple components of the index and CANDH encapsulates the Personal Consumption and Housing data series. CANDH includes data on retail sales, consumption of durable goods, and new housing starts. *It reflects what actually happens in the economy.*
2. [**UMCSENT**](https://fred.stlouisfed.org/series/UMCSENT)^[https://fred.stlouisfed.org/series/UMCSENT]: This measure is of *consumer sentiment* and is produced by the [University of Michigan](https://data.sca.isr.umich.edu/fetchdoc.php?docid=24774)^[https://data.sca.isr.umich.edu/fetchdoc.php?docid=24774] through a survey of consumers. UMCSENT gauges how people are *feeling*. It is perceived as a leading index of economic activity and does not necessarily reflect reality. The [Consumer Confidence Index](https://www.conference-board.org/data/consumerconfidence.cfm)^[https://www.conference-board.org/data/consumerconfidence.cfm] by The Conference Board would have been preferred measure for this macro-trend but it is not a free dataset. 

Below are the two indicators for the 21 years (84 quarters) of this analysis:

```{r macro-trend-plot}
fred_metrics2 %>%
  gather(metric, value, -quarter_date, -quarter) %>%
  ggplot(aes(x = quarter_date, y = value, colour = metric)) +
  geom_line() + 
  geom_point(size = .5) + 
  theme_jrf(users_v = "rstudio") + 
  labs(title = "Macro-Trend Covariates", x = "Quarter", 
       y = "Scaled Index", colour = NULL) + 
  scale_color_manual(values = c(pal538[['blue']], pal538[['green']])) +
  scale_x_date(breaks = as.Date(c("1996-01-01", "2001-01-01", "2006-01-01", "2011-01-01", "2017-01-01")), 
               date_labels = "%Y")
```

We see the indicators follow a similar trend - there is a significant fall during the great recession of 2007-2009 - but the levels differ over the course the time period.

### Seasonality

To account for seasonality in Dish Network sign-ups, we decomposed the customer acquisition time series using [STL](https://www.otexts.org/fpp/6/5)^[https://www.otexts.org/fpp/6/5] (Seasonal and Trend decomposition using Loess). [Loess](https://en.wikipedia.org/wiki/Local_regression)^[https://en.wikipedia.org/wiki/Local_regression] is simply a type of local regression used for estimating non-linear relationships. Below is the decomposition into seasonal, trend, and remainder components:

```{r seasonality-plot}
dish %>%
  bind_cols(seasonality_df) %>%
  select(quarter_date, actual = customers_acquired, seasonal, trend, remainder) %>%
  gather(type, value, -quarter_date) %>%
  mutate(type = str_to_title(type)) %>%
  mutate(type = factor(type, levels = c('Actual', 'Seasonal', 'Trend', 'Remainder'))) %>%
  ggplot(aes(x = quarter_date, y = value)) +
  facet_wrap(~type, ncol = 1, scales = 'free_y') +
  geom_line(colour = pal538[['blue']]) +
  theme_jrf(users_v = "rstudio") + 
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Decomposition to Determine Seasonality", x = NULL, y = "Customers Acquired") +
  scale_x_date(breaks = as.Date(c("1996-01-01", "2001-01-01", "2006-01-01", "2011-01-01", "2017-01-01")), 
               date_labels = "%Y")
```

We scaled the seasonal component and used it as a covariate. The seasonality covariate proved extremely helpful in explaining the shape of the Dish customer acquisition series. 

### Firm-specific

It is reasonable to believe that there were specific actions taken by Dish Network that contributed to the acquisition of customers (or at least the company should hope so), such as product launches or marketing campaigns. There is a specific event that stands out in the time series: the launch of Sling TV in January 2016^[http://www.theverge.com/2015/1/5/7491071/dish-sling-tv-ott-internet-tv-announced-ces-2015]. Sling TV was the first internet TV service to unbundle ESPN from a typical cable/satellite package and was aimed directly at "cord cutters". In many ways it was positioned as a secondary subscription to complement your Netflix or Hulu subscription. Bloomberg reported that Sling TV surpassed 600,000 subscribers in June 2016 and 1 million by October 2016^[https://www.bloomberg.com/news/articles/2016-10-26/dish-s-sling-tv-service-seen-exceeding-1-million-subscribers].

To account for the pop in acquisitions during 2015 (that broke with the plateauing or downward previous trend), we created a Sling TV covariate that increased over the four quarters of 2015.

### Industry-specific

In addition to actions taken by Dish, competitive forces in the TV industry likely contributed to changes in customer acquisition. A covariate we would have wanted to use for this notion is the number of subscribers to TV streaming services such as Netflix, Hulu, or Sony Vue (see Limitations for more details). However, given the idea that Netflix has stolen TV subscribers, or would-be-TV-subscribers in the case of millennials, from traditional cable and satellite companies we used an easily available Netflix dataset: Netflix stock price. Below is the Netflix stock price averaged by quarter and scaled to conform to the other covariates:

```{r netflix-stock-plot}
netflix_stock %>%
  mutate(netflix_stock = na_if(netflix_stock, 0)) %>%
  filter(quarter_date < as.Date("2017-01-01")) %>%
  ggplot(aes(quarter_date, netflix_stock)) + 
  geom_line(colour = pal538[['red']]) + 
  theme_jrf(users_v = "rstudio") + 
  labs(title = "Netflix Stock Price (NFLX)", x = NULL, y = "Scaled Stock Price") +
  scale_x_date(breaks = as.Date(c("1996-01-01", "2001-01-01", "2006-01-01", "2011-01-01", "2017-01-01")), 
               date_labels = "%Y")
```

## WG with Covariates

After determining the WG model was the most appropriate, we then needed to identify which combination of covariates, if any, would produce the best model. We ran the WG with coviarates model for all 32 combinations of the five covariates ($2^5 = 32$), which includes no covariates at all. Below are the top 10 models by BIC (a summary of the remaining 22 can be found in the [Appendix]):

```{r candidate-models-summary}
candidate_models_summary <-
  wg_cov_summary %>%
  mutate(
    CANDH = ifelse(str_detect(model, "CANDH"), "X", NA)
    , UMCSENT = ifelse(str_detect(model, "UMCSENT"), "X", NA)
    , `Seasonality` = ifelse(str_detect(model, "seasonality"), "X", NA)
    , `Sling TV` = ifelse(str_detect(model, "slingbox_effect"), "X", NA)
    , `Netflix` = ifelse(str_detect(model, "netflix_stock"), "X", NA)
  ) %>%
  select(
     CANDH
    , UMCSENT
    , `Seasonality`
    , `Sling TV`
    , `Netflix`
    , `# Params` = params
    , `LL` = neg_ll
    , `MdAPE` = mdape
    , BIC = bic
  )
  
candidate_models_summary %>%
  head(10) %>%
  pander(caption = "Top 10 Weibull-Gamma with Covariate Models by BIC", 
         round = c(0, 0, 0, 0, 0, 0, 0, 4, 0), missing = "", split.table = 100)
```

First, we note that the log-likelihood ($LL$), the median absolute percent error ($MdAPE$), and $BIC$ are all relatively similar for these top models. Second, we note that for all 10 models, the Netflix coviarate appears. Third, we see that the seasonality covariate is present in almost all of the top models. Here we find that Sling TV does not add that much more information. For the second model with all covariates compared to the top model with Sling TV, the $MdAPE$ is slightly lower, $LL$ is nearly the same, and $BIC$ is marginally worse. In fact, the first two models and the second two models nearly look the same graphically. As such, the first and third models above are shown in the incremental and cumulative tracking plots below:

```{r incremental-plot}
instant_plot_data <-
  dish %>%
  select(quarter_date, expected_instant = customers_acquired) %>%
  mutate(model = "Actual") %>%
  bind_rows(
    wg_cov_results %>%
      inner_join(
        wg_cov_summary %>% filter(model %in% c('CANDH & UMCSENT & seasonality & netflix_stock', 
                                               'UMCSENT & seasonality & netflix_stock'))
        , by = "model"
      ) %>%
      select(quarter_date, expected_instant, model) 
  )

ggplot(instant_plot_data) +
  geom_line(aes(x = quarter_date, y = expected_instant, colour = model)) +
  theme_jrf(users_v = "rstudio") +
  labs(title = "Incremental Customer Acquisitions", 
       x = "Quarter", y = "Customers Acquired", colour = NULL) +
  scale_y_continuous(labels = scales::comma) +
  scale_color_manual(values = c(pal538[['red']], pal538[['blue']], pal538[['green']])) + 
  guides(col = guide_legend(nrow = 3)) +
  scale_x_date(breaks = as.Date(c("1996-01-01", "2001-01-01", "2006-01-01", "2011-01-01", "2017-01-01")), 
               date_labels = "%Y")
```

```{r cumulative-plot}
cumulative_plot_data <-
  dish %>%
  mutate(expected_cumulative = cumsum(customers_acquired)) %>%
  select(quarter_date, expected_cumulative) %>%
  mutate(model = "Actual") %>%
  bind_rows(
    wg_cov_results %>%
      inner_join(
        wg_cov_summary %>% filter(model %in% c('CANDH & UMCSENT & seasonality & netflix_stock', 
                                               'UMCSENT & seasonality & netflix_stock'))
        , by = "model"
      ) %>%
      select(quarter_date, expected_cumulative, model) 
  )

ggplot(cumulative_plot_data) +
  geom_line(aes(x = quarter_date, y = expected_cumulative, colour = model)) +
  theme_jrf(users_v = "rstudio") +
  labs(title = "Cumulative Customer Acquisitions",
       x = "Quarter", y = "Cumulative Customers Acquired", colour = NULL) +
  scale_y_continuous(labels = scales::comma) +
  scale_color_manual(values = c(pal538[['red']], pal538[['blue']], pal538[['green']])) + 
  guides(col = guide_legend(nrow = 3)) +
  scale_x_date(breaks = as.Date(c("1996-01-01", "2001-01-01", "2006-01-01", "2011-01-01", "2017-01-01")), 
               date_labels = "%Y")
```

With regards to the model parameters $r$, $\alpha$, and $c$, the table below shows these estimates for the top 10 models by BIC:

```{r top-model-parameter}
wg_cov_params %>%
  left_join(wg_cov_summary, by = "model") %>%
  arrange(bic) %>%
  head(10) %>%
  mutate(
    CANDH = ifelse(str_detect(model, "CANDH"), "X", NA)
    , UMCSENT = ifelse(str_detect(model, "UMCSENT"), "X", NA)
    , `Seasonality` = ifelse(str_detect(model, "seasonality"), "X", NA)
    , `SlingTV` = ifelse(str_detect(model, "slingbox_effect"), "X", NA)
    , `Netflix` = ifelse(str_detect(model, "netflix_stock"), "X", NA)
  ) %>%
  select(
     CANDH
    , UMCSENT
    , `Seasonality`
    , `SlingTV`
    , `Netflix`
    , r
    , alpha
    , c
    , `BIC` = bic
  )  %>%
  pander(caption = "WG Parameter Estimates for Top 10 Models by BIC", 
         round = c(0, 0, 0, 0, 0, 0, 0, 3, 4), missing = "", split.table = 100)
```

We find extremely large values for $r$ and $\alpha$, which confirms our belief that there is not much heterogeneity in the population and why none of the latent-class models were successful. The plot below shows the distribution of the rate parameter $\lambda$ for the first model:

```{r distribution-of-lambda-plot}
wg_cov_params %>%
  left_join(wg_cov_summary, by = "model") %>%
  arrange(bic) %>%
  head(1) %>%
  mutate(lambda = map(r, alpha, .f = function(.x, .y) {as_tibble(rgamma(n =10000, .x, .y))})) %>%
  select(model, lambda) %>%
  unnest() %>%
  ggplot(aes(x = value)) +
  geom_density(colour = pal538[['blue']], fill = pal538[['blue']], alpha = 1/3) +
  theme_jrf(users_v = "rstudio") +
  labs(x = expression(lambda), y = expression(f(lambda)), 
       title = "Estimated Distribution of Lambda", 
       subtitle = "WG with CANDH, UMCSENT, Seasonality, and Netflix Stock")
```

We see that the distribution of $\lambda$ is nearly symmetric with a large number of people having the same rate parameter value. There is some heterogeneity, but there is are not a significant portion of people with high or low $\lambda$'s. In other words, we would not consider using a pure Weibull with covariates upon seeing plot, but segments do not appear fruitful either.

Next, we review the hazard function of the WG with covariates model. We are surprised to find values of $c > 2$ as this is rare. A value of $c > 1$ whose hazard function is (monotonically) increasing implies that purchase rate increases over time at the individual level. The functions are not monotonically increasing due to the covariates. The plot below shows the hazard function for the top 9 models, all with increasing hazard functions. The implication is that given you have not subscribed to Dish yet, the probability that you subscribe now increases over time.

```{r hazard-function-plot}
wg_cov_results %>%
  mutate(
    f_t = expected_instant / population
    , F_t = expected_cumulative / population
    , h_t = f_t / (1 - F_t)
  ) %>%
  inner_join(
    wg_cov_summary 
    , by = "model"
  ) %>%
  arrange(bic) %>%
  mutate(rnk = dense_rank(mdape)) %>%
  filter(rnk <= 9) %>%
  mutate(model = paste0("Model ", rnk)) %>%
  ggplot(aes(t, h_t)) + 
  geom_line(colour = pal538[['blue']]) + 
  facet_wrap(~model, nrow = 3) +
  theme_jrf(users_v = "rstudio") +
  labs(title = "Hazard Function for Top 9 Models", y = expression(h(t)), 
       x = expression(t))
```

# Results

## Final Model

As our final model, we select the candidate model with the lowest BIC. To ensure that an additional parameter is necessary, we use the likelihood ratio test ($df = 1$) with the next-best-model that does not include the covariate CANDH. We find a very small $p$-value and conclude that the models are not the same and thus the model with four covariates is valuable and confirm this as our final model.

```{r lrt-table}
candidate_models_summary %>% 
  filter(row_number() %in% c(1, 4)) %>% 
  mutate(
    LRT = 2 * (LL - lead(LL))
    , `$p$-value` = pchisq(LRT, df = 1, lower.tail = FALSE)
  ) %>%
  select(-MdAPE, -BIC) %>%
  pander(caption = "Likelihood Ratio Test for Next-Best-Model", missing = "", 
         round = c(0,0,0,0,0,0,1,4,4), split.table = Inf)
```

This model does not include the artificial Sling TV coviariate. We are glad to remove this coviariate as it will not be helpful for future prediction. Below is a summary of our final model:

```{r final-model-value}
wg_cov_params %>%
  left_join(wg_cov_summary, by = "model") %>%
  arrange(bic) %>%
  head(1) %>%
  mutate(
    r = round(r, 0)
    , alpha = round(alpha, 0)
    , c = round(c, r)
    , LL = round(neg_ll, 1)
    , BIC = round(bic, 1)
  ) %>%
  mutate(cov_betas = map(cov_betas, round, 4)) %>%
  select(r, `$\\alpha$` = alpha, c, `Covariates` = cov_names, `Covariate $\\beta$s` = cov_betas, LL, BIC) %>%
  gather(`&nbsp;`, Value) %>% 
  pander(caption = "Summary of Final Model", split.cells = c(20, 45))
```

## Forecast 2017

In order to make predications of Dish Network's subscriber acquisition in 2017, the covariates needed to be carried into the future. The table below summarizes how each covariates values were estimated for 2017:

|Covariate|Implementation|
|:---------:|:---------------------------------------------:|
|Macro-Trends|Index values for CAND and UMCSENT were released for the months in Q1 2017 and were used. For 2017Q2 to 2017Q4, we created an ARIMA model\endnote{\url{https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average}} using all the quarterly data from 1996Q1 to 2017Q1 and predicted 3 periods ahead. An automatic method for selecting the parameters of the ARIMA model was used.|
|Seasonality|Using the STL decomposition, the time series was forecasted 4 periods ahead and the seasonal component was used.|
|Firm-Specific|Not necessary as not included in model|
|Industry-Specific|Netflix stock prices from 2017Q1 were used and then an (automatically selected) ARIMA model using all the quarterly stock prices from 1996Q1 to 2017Q1 were used to predict 3 periods ahead.|

The plot below shows the forecasts of the four covariates:

```{r forecast-covariates-plot}
bind_rows(dish_model_data,
          dish_model_data %>% 
            tail(1) %>% 
            mutate(t = t +1)
          , forecast_data) %>%
  select(quarter_date, t, CANDH, UMCSENT, seasonality, netflix_stock) %>%
  mutate(netflix_stock = na_if(netflix_stock, 0)) %>%
  gather(covariate, value, -quarter_date, -t) %>%
  mutate(forecast = t >= 85) %>%
  mutate(covariate = factor(covariate, levels = c("CANDH", "UMCSENT","seasonality","netflix_stock"))) %>%
  ggplot(aes(x = quarter_date, y = value, colour = covariate, linetype = forecast)) +
  geom_line() +
  scale_colour_manual(values = c(pal538[['red']], pal538[['blue']], pal538[['green']], pal538[['dkgray']])) +
  theme_jrf(users_v = "rstudio") +
  labs(title = "Forecasts of Covariates", x = "Quarter", y = "Scaled Value", 
       colour = NULL) + 
   guides(linetype = FALSE) +
  scale_x_date(breaks = as.Date(c("1996-01-01", "2001-01-01", "2006-01-01", "2011-01-01", "2017-01-01")), 
               date_labels = "%Y")
```

Incorporating these forecasts, we forecast the number of customers acquired in each quarter to be 

```{r forecast-customer-acquisition-table}
dish_actual_and_forecast %>% 
  filter(metric == "Incremental" & forecast == TRUE) %>%
  mutate(
    Quarter = quarter(quarter_date, with_year = TRUE)
    , Quarter = str_replace(Quarter, "\\.","Q")) %>%
  select(Quarter, `Customers Acquired` = value) %>%
  pander(caption = "Forecasted Customer Acquisition", round = c(0, 0))
```

The figure below shows the incremental and cumulative tracking plots with the forecasts:

```{r fig.width = 7, fig.height = 3.7}
dish_actual_and_forecast %>%
  bind_rows(
    dish_actual_and_forecast %>% 
      filter(quarter_date == "2016-10-01" & model == "Model") %>% 
      mutate(quarter_date = quarter_date, forecast = TRUE)
  ) %>%
  ggplot(aes(x = quarter_date, y = value, colour = model, linetype = forecast)) +
  geom_line() + 
  facet_wrap(~ metric, scales = 'free') +
  theme_jrf(users_v = "rstudio") +
  guides(linetype = FALSE) + 
  labs(title = "Dish Network Subscriber Acquisition", y = "Customers", 
       x = "Quarter", colour = NULL) +
  scale_y_continuous(labels = scales::comma) +
  scale_colour_manual(values = c(pal538[['red']], pal538[['blue']])) +
  scale_x_date(breaks = as.Date(c("1996-01-01", "2001-01-01", "2006-01-01", "2011-01-01", "2017-01-01")), 
               date_labels = "%Y")
```

# Limitations

1. **Population Size**. In this analysis we used the assumption that the overall customer population (N) was 60M. In a subsequent analysis we would attempt to implement (1) a truncated model and (2) vary N to identify its impact on parameter estimates.
2. **Better Metric for Industry-Specific Covariate**. We would have preferred to use the number of subscribers for all US TV streaming services. A secondary measure could have been revenue for Netflix, but this was only readily available for recent time period. A future analysis would implement subscriber information from a market data firm such as [Second Measure](https://secondmeasure.com/)^[https://secondmeasure.com/].
3. **Jump in 2016Q3 - 2016Q4 Acquisitions**. The Sling TV covariate sought to capture the impact of subscribers to this new service. The artificial covariate seemed reasonable to cover 2015, but given the drop-off in Q1 and Q2 and lack of product news in 2016 did not seem reasonable to cover 2016. However, the two periods at the end of 2016 indicate a phase shift may have occurred and could cause our forecasts, which are much lower, to be significantly different.
4. **Forecasting Covariates**. The fact that we forecasted covariates is likely to cause issues with our forecast of the three remaining periods of 2017.
5. **Segmenting $c$**. While we attempted segments with Weibull and Weibull-Gamma models, we did not attempt to segment $c$ in isolation while keeping $\lambda$ and $r$ fixed.

# Appendix

## Bottom 22 Models

```{r}
candidate_models_summary %>%
  tail(22) %>%
  pander(caption = "Bottom 22 Weibull-Gamma with Covariate Models by BIC", 
         round = c(0, 0, 0, 0, 0, 0, 0, 4, 0), missing = "", split.table = 100)
```

## Technical Notes

All MLE optimization of the Weibull-Gamma with covariates model was performed using [nlminb](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/nlminb.html)^[https://stat.ethz.ch/R-manual/R-devel/library/stats/html/nlminb.html].

The full source code that created this document can be found in the [Github repo](https://github.com/jrfarrer/mktg776_p2/).

\renewcommand{\notesname}{References}
\theendnotes

